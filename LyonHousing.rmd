---
title: "Phase2"
author: "MohammadAli Olama , 98100497"
date: "2/10/2022"
output: html_document
---

```{r , include=FALSE}
bool = TRUE
```



```{r , message = FALSE , include=FALSE}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

# install.packages("readr")
# install.packages("stringr")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("ggthemes")
# install.packages("broom")
# install.packages("ggpubr")
# install.packages("quantmod")
# install.packages("rcompanion")
# install.packages("heatmaply")
# install.packages("rjson")
# install.packages("BSDA")
# install.packages("lemon")

library(readr)
library(stringr)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(fastR2)
library(mosaicData)
library(broom)
library(ggpubr)
library(data.table)
library(quantmod)
library(rcompanion)
library(reshape2)
library(heatmaply)
library(rjson)
library(BSDA)
library(lemon)

```


```{r setup, include=FALSE}


knitr::opts_chunk$set(echo = TRUE)
knit_print.data.frame=lemon_print
```



# part i
first read the file :

```{r, echo=bool}
a = read.csv("lyon_housing.csv" , encoding = "UTF-8")
```

sort it with respect to date_transaction and then its price :
```{r, echo=bool , render = lemon_print}
a = a[order(a$date_transaction , a$price),]
head(a)
```

let's see how many invalid or NA values are there in the data set per column:

```{r, echo=bool}

colSums(is.na(a))

```

it seems that more than 35% of samples suffer from lack of "surface_effective_usable" value. now if we replace this unknown values with other values(e.g mean or median or ...) we may produce a great error terms. so it's better to think we don't have this column and use other columns.

also 143 houses don't have latitude - longitude values. we can skip this houses in our calculations(if we need to use their location). because we have about 40516 samples and 143 is small in relative to 40516.

at first, we remove "suface_effective_useable" column from all samples. this column contains about 14000 missing values so it is not useable for us.

second we exclude samples with longitude or latitude missing values(143 samples)
```{r ,echo=bool}
effective_usable = a$surface_effective_usable
a = a[,c(1:5 , 7:14)]
a= a[complete.cases(a) , ]

```


then read json file of stations:

```{r ,echo=bool}
stations = fromJSON(file = "station_coordinates.json")
namess = c()
longitudes = c()
latitudes= c()
group = c()
models = c()
for (i in 1:11){
  AA = stations[i]
  AA = as.data.frame(AA)
  di = dim(AA)
  for (j in 1:di[1]){
    namess = append(namess , AA[j,1])
    longitudes = append(longitudes , AA[j,3])
    latitudes = append(latitudes , AA[j,2])
    group= append(group , i)
    if (i<=4){
      models = append(models , "train")
    }else{
      models = append(models , "tramva")
    }
  }
}

stations = data.frame(namess , latitudes , longitudes , group , models)
stations$models = as.factor(stations$models)
stations$group = as.factor(stations$group)
colnames(stations) = c("name" , "latitude" , "longitude" , "group" , "model")
```

```{r , render=lemon_print ,echo=bool}
head(stations)
```

map of the stations :

```{r}
ggplot(data = stations)+geom_point(mapping = aes(x=latitude,y=longitude, color=group))
```



lets add 3 more columns.these columns show the date of transaction in year, month, day. we will use them as categorical values.
```{r, echo=bool}

d = as.POSIXct(a$date_transaction , format="%Y/%m/%d")
y =format(d , format="%Y")
m = format(d , format="%m")
d = format(d , format="%d")
for (i in 16:21){
  st1 = paste("00" ,i , sep = "")
  st2 = paste("20" ,i , sep = "")
  y[y==st1] = st2
}

a$year = as.character(y)
a$month = as.character(m)
a$day = as.character(d)
```

and add 3 more columns.these columns show the date of construction in year, month, day. we will use them as categorical values:
```{r,echo=bool}

d2 = as.POSIXct(a$date_construction , format="%Y/%m/%d")
y2 =format(d2 , format="%Y")
m2 = format(d2 , format="%m")
d2 = format(d2 , format="%d")
for (i in 0:9){
  st1 = paste("000" ,i , sep = "")
  st2 = paste("200" ,i , sep = "")
  y2[y2==st1] = st2
}
for (i in 10:23){
  st1 = paste("00" ,i , sep = "")
  st2 = paste("20" ,i , sep = "")
  y2[y2==st1] = st2
}
for (i in 85:99){
  st1 = paste("00" ,i , sep = "")
  st2 = paste("19" ,i , sep = "")
  y2[y2==st1] = st2
}

a$cons_year = as.character(y2)
a$cons_month = as.character(m2)
a$cons_day = as.character(d2)

```

and add 1 more columns.these column shows the distance of each house to its nearest station(kilometers). 
```{r }
di =dim(stations)
a$station_distance = rep(Inf , each=40373)

for (i in 1:di[1]){
  lat1 = a$latitude 
  lat2 = stations$latitude[i]
  lat1 = lat1 / ((180/pi))
  lat2 = lat2 / ((180/pi))
  
  lon1 = a$longitude 
  lon2 = stations$longitude[i]
  
  lon1 = lon1 / ((180/pi)) 
  lon2 = lon2 / ((180/pi)) 
  
  d = 3963.0 * acos((sin(lat1) * sin(lat2)) + cos(lat1) * cos(lat2) * cos(lon2-lon1))
  d = 1.609344 * d 
  d2 = pmin(a$station_distance , d)
  a$station_distance = d2
}

```

```{r, render = lemon_print ,echo=bool}
head(a)
```

let's talk about columns of this dataset :

## columns

#### data_transacton:
this column ranges from 2016/07/01 to 2021/06/30.
we will plot a map of houses acording to their location with respect to their year of transaction later.


#### type_purchase:
type_purchase : it's a 2-state factor variable with values "ancien" and "VEFA"

```{r, echo=bool}
table(a$type_purchase)
```

#### type_property:
it's a 2-state factor variable with values "appartement" and "maison"

```{r, echo=bool}
table(a$type_property)

```


#### rooms_count:
it's an integer value variable ranges from 1 to 6 rooms.
```{r, echo=bool}
table(a$rooms_count)
hist(a$rooms_count ,main="Distribution of rooms_count",xlab = "rooms_count", col="red", breaks = seq(0.25 , 6.5 , 0.5))
```


#### surface_housing:
 it's an integer value variable ranges from 20 to 300
 
```{r , echo=bool}
hist(a$surface_housing , breaks = seq(-10,350 , 5) , col="green" , main="Distribution of surface_housing" , xlab = "surface_housing")
```
 
 
the mean and standard error of this column is:
```{r, echo=bool}
mea = mean(a$surface_housing)
s = sd(a$surface_housing)

print(paste("mean=" , mea , ", sd=" , s))

```


now if we plot it and match a normal curve on it :
```{r, echo=bool}
var = var(a$surface_housing)
his = hist(a$surface_housing , breaks = seq(0,300,5) , col="yellow" , main="Distribution of surface_housing" , xlab = "surface_housing")
xfit = seq(min(a$surface_housing) , max(a$surface_housing) , length=length(a$surface_housing))
yfit = dnorm(xfit , mean = mea, sd = s) * diff(his$mids[1:2] * length(a$surface_housing))

lines(xfit , yfit , col="blue" , )

```


#### surface_effective_usable: 
it's float value variable ranges from 2.81(!) to 270.

```{r, echo=bool}
hist(effective_usable , breaks = seq(0,300 , 10) , col = "pink")
```

we saw in previous assumptions that this columns seems to be useless (because of great number of NA in it) and so we have removed it from dataset.



#### surface_terrain:
 it's an integer value variable ranges from 0 to 900. if we plot :
 
```{r , echo=bool}
hist(a$surface_terrain , col = "purple" , main="Distribution of surface_terrain" , xlab = "surface_terrain")
```
 
it seems strange!. lets see how many houses have a yard(e.g have non-zero surface_terrain area)

```{r , echo=bool}
t = a$surface_terrain
t[t>0] = 1
t = as.factor(t)
table(t)
```
so about 1.4% of the houses has a yard!!!
let's have look at distribution of non-zero ones :

```{r,echo=bool}
t2 = a[a$surface_terrain!=0 , ]
hist(t2$surface_terrain , breaks = seq(0 , 1000 , 10) , col = "purple" , main="Distribution of surface_terrain" , xlab = "surface_terrain")
```



#### parkings_count: 
it's an integer value variable ranges from 0 to 3
```{r, echo=bool}
table(a$parkings_count)
```


#### price:
it's a float value variable ranges from 24000.00 to 2780000.00.


if we draw histogram of houses with respect to their price :

```{r, echo=bool}
hist(a$price , breaks = seq(20000 , 2800000 , 1000) ,  main="Distribution of price" , xlab = "price")
```


it seems it has a lot of outliers. if we count number of house with price greater the 1m :
```{r, echo=bool}
colSums(a[c("price")] > 1000000)
```
so the number of outlier samples is small.(in relative to all samples).
let's see how these outlier affect the mean of price :
```{r , echo=bool}
m1 = mean(a$price , na.rm=TRUE)
m1
```

```{r , echo=bool}
b1 = a$price
b1 = b1[!is.na(b1) & b1 <= 1000000]
m2 = mean(b1)
m2
```
so the effect of outliers on mean of prices is about:
```{r , echo=bool}
m1 - m2
```




#### address:
 it's character value variable.


#### district:
 it's a 10-state factor variable showing districts of lyon.
 let's see distribution of houses according to these districts:
```{r, echo=bool}
table(a$district)
```


#### latitude, longitude: 
float value variables showing location of the house.
```{r, echo=bool}
hist(a$latitude ,  main="Distribution of latitude" , xlab = "latitude")
hist(a$longitude, main="Distribution of longitude" , xlab = "longitude")

```



now let's have a look at a map :

```{r, echo=bool}
a$district = as.factor(a$district)
ggplot(data = a)+geom_point(mapping = aes(x=latitude,y=longitude, color=district))
```

this plot will help us to have a better understanding of lyon's districts.

another map :

```{r}
a$type_property = as.factor(a$type_property)
ggplot(data = a)+geom_point(mapping = aes(x=latitude,y=longitude, color=type_property))
a$type_property = as.character(a$type_property)
```


















#### date_construction: 
shows time of construction . by using construction_year which we have added to the dataset in the begining, we plot a histogram :

```{r , echo=bool}
a$cons_year = as.factor(a$cons_year)
barplot(table(a$cons_year))
```

wait what ?! constuction date of some the house is for future :| (year 2022 and 2023!)
let's count them!

```{r , echo=bool}
table(a$cons_year)
```
maybe some of these values are invalid dates (e.g construction date later than 2021/06/30 and they are not "VEFA" ):

```{r , echo=bool}
a$cons_year = as.character(a$cons_year)
a$cons_year = as.numeric(a$cons_year)
a$cons_month = as.numeric(a$cons_month)
colSums(a[c("cons_year")] == 2021 & a[c("cons_month")] >7 & a[c("type_purchase")]=="ancien")
a$cons_year = as.character(a$cons_year)
a$cons_month = as.character(a$cons_month)


```

it seems that these dates are wrong and invalid.(55 invalid dates) so we should be aware of this in later calculations.

another map:

```{r}
ggplot(data = a)+geom_point(mapping = aes(x=latitude,y=longitude, color=cons_year))
```
it seems that the city can be divided in two parts of old-built and new-built houses.



## other useful data:


let's have a look at number of transaction per district :
```{r, echo=bool}
a$year = as.factor(a$year)
table(a$district)
```

the highest number of houses sold in these areas belongs to Villeurbanne.the lowest number of houses belongs to Lyon 2e Arrondissement.



let's have a look at number of transaction per year :
```{r, echo=bool}
table(a$year)

```
as you can see, most of transactions take place in 2017 and year 2021 has the least transaction count.(we should notice that our data for years 2016 and 2021 covers only 6 month of each year, but for other years, all 12 months are covered.)


let's have a look at number of transaction per year & transaction. :
```{r, echo=bool}
table(a$district , a$year)
a$year = as.character(a$year)
```



let's calculate mean of transaction price of each district according to different years :
```{r, echo=bool}
districts = c("Lyon 1er Arrondissement" , "Lyon 2e Arrondissement" ,
              "Lyon 3e Arrondissement" , "Lyon 4e Arrondissement" ,
              "Lyon 5e Arrondissement" , "Lyon 6e Arrondissement" ,
              "Lyon 7e Arrondissement" , "Lyon 8e Arrondissement" ,
              "Lyon 9e Arrondissement" , "Villeurbanne")

years = c("2016" , "2017" , "2018" , "2019" , "2020" , "2021")

means = c()
for (di in districts) {
  for (ye in years) {
    t = mean(a[a$year==ye & a$district ==di , 'price'])
    means = c(means , t)
  }
}
values = matrix(means,ncol=6,byrow=TRUE)
colnames(values) = years
rownames(values) = districts
values2 = as.table(values)
values2
```
 it seems that district "Villeurbanne" has the lowest prices(actually mean of prices) and district "Lyon 6e Arrondissement" has the highest prices.
 
 

 
 
 now if we plot this table, we can see increase and decreases in price in another format :
 
```{r, echo=bool}


values = data.frame(values)
colnames(values) = years
rownames(values) = districts
values = transpose(values)
values = data.frame(years , values)

colnames(values) = c("years" , districts)
rownames(values) = years



df2 = data.frame(district=rep(districts, each=6),
                year=rep(years , times = 10) , price =means)


ggplot(df2, aes(x=year, y=price, group=district)) +
  geom_line(aes(color=district))+
  geom_point(aes(color=district))
```
 
let's calculate differences and Relative differences of prices for each district from year 2016 to 2021 and see which districts has minimum or maximum (relative) differences :
```{r, echo=bool}

xcv = c()
xcv2 = c()
for (di in districts) {
  a1 = df2[df2$district==di & df2$year=="2016" , 'price']
  a2 = df2[df2$district==di & df2$year=="2021" , 'price']
  percent = (a2-a1)/ a1
  percent2 = (a2-a1)/ 5
  xcv = c(xcv , percent)
  xcv2 = c(xcv2 , percent2)
}

```

Differences:

```{r , echo=bool}

names(xcv2) = districts
xcv2
B1 = which.min(xcv)
B2 = which.max(xcv)
print(paste("minimum increase in prices is:" ,xcv2[B1], " and is for district:" , names(which.min(xcv2)) ))
print(paste("maximum increase in prices is:" ,xcv2[B2], " and is for district:" , names(which.max(xcv2)) ))


```



Relative differences :

```{r , echo=bool}
names(xcv) = districts
xcv
A1 = which.min(xcv)
A2 = which.max(xcv)
print(paste("minimum increase rate is:" ,xcv[A1], " and is for district:" , names(which.min(xcv)) ))
print(paste("maximum increase rate is:" ,xcv[A2], " and is for district:" , names(which.max(xcv)) ))
```

we can be more specific about figure above :

in this figure , we set 2016/1 (jan) to be 1 and for each month of each yaer we calculate time = 12*(year-2016) + month.
then calculate mean of prices of that time.
and then plot.

```{r, echo=bool}
districts = c("Lyon 1er Arrondissement" , "Lyon 2e Arrondissement" ,
              "Lyon 3e Arrondissement" , "Lyon 4e Arrondissement" ,
              "Lyon 5e Arrondissement" , "Lyon 6e Arrondissement" ,
              "Lyon 7e Arrondissement" , "Lyon 8e Arrondissement" ,
              "Lyon 9e Arrondissement" , "Villeurbanne")

years = c("2016" , "2017" , "2018" , "2019" , "2020" , "2021")
month = c("01" ,"02","03","04","05","06","07","08","09","10","11","12")

means = c()
dis = c()
yearss = c()
months = c()
for (di in districts) {
  for (ye in years) {
    for (mo in month) {
      t = mean(a[a$year==ye & a$month==mo & a$district ==di , 'price'])
      means = c(means , t)
      dis = append(dis , di)
      yearss = append(yearss , ye)
      months = append(months , mo)
    }
  
  }
}
yearss = as.numeric(yearss)
yearss = yearss - 2016
months = as.numeric(months)

time = 12 * yearss + months

nbv = data.frame(means , dis , yearss , months , time , stringsAsFactors = F)
nbv2 = nbv[complete.cases(nbv) , ]


ggplot(nbv2, aes(x=time, y=means, group=dis)) +
  geom_line(aes(color=dis))+
  geom_point(aes(color=dis))

```

this plot has much more information but it is really ugly. the best thing we can do is to plot seperatly for each district(which we are not going to do.)



__________________________________________________________________________________________________"


now, as we promised, if we plot a map of houses according to their location with respect to their year of transaction :
```{r, echo=bool}
a$year = as.factor(a$year)
ggplot(data = a)+geom_point(mapping = aes(x=latitude,y=longitude, color=year))
a$year = as.character(a$year)
```


## FIND RELATION BETWEEN COLUMNS

let's plot a heatmap which shows the correlations between columns of the dataset. we just use numerical columns :

```{r , echo=bool}
new_a = a[c(4,5,6,7 ,11,12,17, 8)]
new_a$cons_year = as.numeric(new_a$cons_year)

corr = round(cor(new_a),2) 

mcorr = melt(corr , na.rm = TRUE)

ggplot(data = mcorr, aes(x=Var1, y=Var2,fill=value)) +
geom_tile() +
   scale_fill_gradient(low = "gray", high = "red") +
  geom_text(aes(Var2, Var1, label = value),
          color = "black", size = 4) +theme(axis.text.x = element_text(angle = 20, vjust = 1, 
    size = 9, hjust = 1))

```

now let's investigate this heatmap.

1- it seems that "rooms_count" and "surface_housing" are positively correlated (0.84). it seems logical: as number of rooms grow, total area covered by the house will increase.

2- another noticeable correlation, is between "price" and "surface_housing". it is also reasonable to think that the vaster a house is, the more expensive it becomes.

3- another noticeable correlation, is between "price" and "rooms_count". with conclusions above, it's reasonable to think with more rooms, there will be more area covered by the house , and the vaster it is, the more expensive it becomes. but this correlation is less noticeable than two above.


let's plot some scatters to see :

1- 
```{r, echo=bool}
r1 = (a[c("rooms_count")]==1)
r2 = (a[c("rooms_count")]==2)
r3 = (a[c("rooms_count")]==3)
r4 = (a[c("rooms_count")]==4)
r5 = (a[c("rooms_count")]==5)
r6 = (a[c("rooms_count")]==6)

ggplot() + geom_point(mapping = aes(a$surface_housing , a$rooms_count))+
    scale_y_continuous(breaks = c(0,1,2,3,4,5,6)) +
  geom_point(mapping = aes(mean(a$surface_housing[r1]) , 1) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r2]) , 2) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r3]) , 3) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r4]) , 4) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r5]) , 5) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r6]) , 6) , color="red")+ xlab("surface_housing") + ylab("rooms_count")
```

now we can see the correlation better. the red dots are mean of surface_housing in each row. so we can see that when number of rooms increases, surface_housing increase too.


2- 

```{r, echo=bool}
ggplot() + geom_point(mapping = aes(a$surface_housing , a$price))+xlab("surface_housing")+ylab("price")
```

it seems that the price of a house grows as surface of the house grows.(in total, some outliers may not behave like this.)


3- 

```{r, echo=bool}

r1 = (a[c("rooms_count")]==1)
r2 = (a[c("rooms_count")]==2)
r3 = (a[c("rooms_count")]==3)
r4 = (a[c("rooms_count")]==4)
r5 = (a[c("rooms_count")]==5)
r6 = (a[c("rooms_count")]==6)


ggplot() + geom_point(mapping = aes(a$price , a$rooms_count)) +
  scale_y_continuous(breaks = c(0,1,2,3,4,5,6))+ geom_point(mapping = aes(mean(a$price[r1]) , 1) , color="red")+ geom_point(mapping = aes(mean(a$price[r2]) , 2) , color="red")+ geom_point(mapping = aes(mean(a$price[r3]) , 3) , color="red")+ geom_point(mapping = aes(mean(a$price[r4]) , 4) , color="red")+
geom_point(mapping = aes(mean(a$price[r5]) , 5) , color="red")+
geom_point(mapping = aes(mean(a$price[r6]) , 6) , color="red")+xlab("price") +ylab("rooms_count")

```

now we can see the correlation better. the red dots are mean of price in each row. so we can see that when number of rooms of a house increases, price of the house increase too.(but it is less obvious than the increament in surface_housing).



#### we will do some regression tests in part ii to prove these assumptions.

## ADD EXTRA COLUMNS :
we have added some columns in the beginning of our work. here we add another column :

let's see how much does 1m^2 area cost(e.g land price) for each house?

```{r, echo=bool}
a$area_price_ratio = a$price / a$surface_housing

```
maximum ratio is :
```{r, echo=bool}
max(a$area_price_ratio)
```
minimum ratio is:
```{r, echo=bool}
min(a$area_price_ratio)

```

mean of all ratios is :

```{r, echo=bool}
mean(a$area_price_ratio)

```
let's plot map of all houses again. this time color of each point will reflect the area_price_ratio
```{r,echo=bool}

ggplot()+geom_point(mapping = aes(x=a$latitude,y=a$longitude , color = a$area_price_ratio) )+scale_color_distiller(palette = 'Spectral')
```



let's see its histogram :
```{r, echo=bool}
hist(a$area_price_ratio , breaks = seq(0 , 36000 , 100))

```

it looks like a normal curve.but, it seems that we have a lot of outliers.

lets see how many house have a ratio greater than 10000 :

```{r, echo=bool}
colSums(a[c("area_price_ratio")] > 10000)
```

so the number of outlier samples is small.(in relative to all samples).
let's see how these outlier affect the mean of price :
```{r , echo=bool}
m1 = mean(a$area_price_ratio , na.rm=TRUE)
m1
```

```{r , echo=bool}
b1 = a$area_price_ratio
b1 = b1[!is.na(b1) & b1 <= 10000]
m2 = mean(b1)
m2
```
so the effect of outliers on mean of prices is about:
```{r , echo=bool}
m1 - m2
```
so it seems that those outliers does not have strong effect on the land price. 


in the end let's plot another figure :
this figure will show how "area_price_ratio" changed during time for each district. 

```{r, echo=bool}
districts = c("Lyon 1er Arrondissement" , "Lyon 2e Arrondissement" ,
              "Lyon 3e Arrondissement" , "Lyon 4e Arrondissement" ,
              "Lyon 5e Arrondissement" , "Lyon 6e Arrondissement" ,
              "Lyon 7e Arrondissement" , "Lyon 8e Arrondissement" ,
              "Lyon 9e Arrondissement" , "Villeurbanne")

years = c("2016" , "2017" , "2018" , "2019" , "2020" , "2021")
month = c("01" ,"02","03","04","05","06","07","08","09","10","11","12")

means = c()
dis = c()
yearss = c()
months = c()
for (di in districts) {
  for (ye in years) {
    for (mo in month) {
      t = mean(a[a$year==ye & a$month==mo & a$district ==di , 'area_price_ratio'])
      means = c(means , t)
      dis = append(dis , di)
      yearss = append(yearss , ye)
      months = append(months , mo)
    }
   
  }
}
yearss = as.numeric(yearss)
yearss = yearss - 2016
months = as.numeric(months)

time = 12 * yearss + months

nbv = data.frame(means , dis , yearss , months , time , stringsAsFactors = F)

nbv2 = nbv[complete.cases(nbv) , ]



ggplot(nbv2, aes(x=time, y=means, group=dis)) +
  geom_line(aes(color=dis))+
  geom_point(aes(color=dis))




```

in this figure , we set 2016/1 (jan) to be 1 and for each month of each yaer we calculate time = 12*(year-2016) + month.
then calculate mean of (area_price_ratio) of that time for each district.
and then plot.

we use this figure in the next part.















# part ii

### 1

the first and most important thing we can do, is to do a regression for price according to other features. this regression can be used for estimating price for other houses :

```{r,echo=bool}
a$cons_year = as.numeric(a$cons_year)
fit3 = lm(price~rooms_count + surface_housing + surface_terrain + parkings_count + cons_year + station_distance  , data=a)
summary(fit3)
a$cons_year = as.character(a$cons_year)

```
the residual error of this regression is 93600.let's use type_property as a dummy variable :

```{r,echo=bool}
a$cons_year = as.numeric(a$cons_year)
# a2 = a[(a$price < 750000) & (a$type_property=="appartement") ,]
a2 = a
a2$type_property =as.factor(a2$type_property)

fit3 = lm(price~rooms_count + surface_housing + surface_terrain + parkings_count + cons_year + station_distance + type_property , data=a2)
summary(fit3)
a$cons_year = as.character(a$cons_year)
```
nothing changed! let's use districts as dummy variables :

```{r,echo=bool}
a$cons_year = as.numeric(a$cons_year)
# a2 = a[(a$price < 750000) & (a$type_property=="appartement") ,]
a2 = a
a2$type_property =as.factor(a2$type_property)
a2$district = as.factor(a2$district)
fit3 = lm(price~rooms_count + surface_housing + surface_terrain + parkings_count + cons_year + station_distance + type_property + district , data=a2)
summary(fit3)
a$cons_year = as.character(a$cons_year)
```
it got really better!(residual error is now 85100). but i think it can get even better. remember that we have some outliers with really high prices.let us not use them :


```{r,echo=bool}
a$cons_year = as.numeric(a$cons_year)
a2 = a[(a$price < 750000),]
a2$type_property =as.factor(a2$type_property)
a2$district = as.factor(a2$district)
fit3 = lm(price~rooms_count + surface_housing + surface_terrain + parkings_count + cons_year + station_distance + type_property + district , data=a2)
summary(fit3)
a$cons_year = as.character(a$cons_year)
```
again it got really better!(residual error is now 73790). but i think it can get even better. remember that we have some outliers with really high prices.let us not use them.also use rooms_count and parking_count as dummy variables :

```{r,echo=bool}
a$cons_year = as.numeric(a$cons_year)
a2 = a[(a$price < 750000),]
a2$type_property =as.factor(a2$type_property)
a2$district = as.factor(a2$district)
a2$parkings_count = as.factor(a2$parkings_count)
a2$rooms_count = as.factor(a2$rooms_count)
fit3 = lm(price~rooms_count + surface_housing + surface_terrain + parkings_count + cons_year + station_distance + type_property + district , data=a2)
summary(fit3)
a$cons_year = as.character(a$cons_year)
```

### 2

remember this :
```{r , echo=bool}
names(xcv) = districts
xcv
A1 = which.min(xcv)
A2 = which.max(xcv)
print(paste("minimum increase rate is:" ,xcv[A1], " and is for district:" , names(which.min(xcv)) ))
print(paste("maximum increase rate is:" ,xcv[A2], " and is for district:" , names(which.max(xcv)) ))
```

so it seems that in district "Lyon 1er Arrondissement" , mean of prices in year 2021 is almost 1.56 times mean of prices in year 2016.

so we run a hypothesis test:

H0 : mean of prices of year 2021 is 1.5 times mean of prices in year 2016.(in Lyon 1er Arrondissement)

H1 : otherwise.

```{r,echo=bool}
d1er = a[a$district=="Lyon 1er Arrondissement", ]
d1er2016 = d1er[d1er$year=="2016" ,]
d1er2021 = d1er[d1er$year=="2021", ]


```

```{r,echo=bool}

z.test(d1er2021$price, y = 1.5*d1er2016$price , alternative = 'two.sided', mu = 0, sigma.x = sd(d1er2021$price), sigma.y = sd(1.5*d1er2016$price),conf.level = 0.95)

```

so, as the z_test shows, the p-value is significant, thus we can not reject our null hypothesis(we don't accept it neither.)

we can run such a test for other districts and other years too.(of course with different multiplication rate)



### 3

let's have look at mean price of apartments and maisons :

```{r,echo=bool}
r1 = a[a$type_property == "appartement" , ]
r2 = a[a$type_property == "maison" , ]

mean(r1$price)
mean(r2$price)
```
obviously these two values can not be equal.(you can run a test and see p-value < 2.2e-16. )

but, we can a test to see how much is mean of maisons is greater than mean of apartements ?

if we divide this two value, the ratio is :
```{r,echo=bool}
mean(r2$price) / mean(r1$price)
```

so let's run a test to see if mean price of maisons is 2.4 times greater than mean price of aprtments :

H0 : mean(maison) = 2.4 * mean(apartment)

H1 : otherwise.

```{r,echo=bool}
 z.test(r2$price, y = 2.4*r1$price , alternative = 'two.sided', mu = 0, sigma.x = sd(r2$price), sigma.y = sd(2.4*r1$price),conf.level = 0.95)

```

the p-value is significant and so we can not reject the nul hypothesis.

notice that if we run the test with a multiple of 2.5(instead of 2.4) then the null hypothesis would be rejected.



### 4

we calculated distance of each house to its nearest station in part I . let's use this feature. first let's have a look at the distribution of this feature :

```{r,echo=bool}
hist(a$station_distance , breaks = seq(0 , 4 , 0.05),col = "red" , main="Distribution of station_distance" , xlab = "station_distance")
```


now let's count number of houses for which the nearest station is located in a radius of at most 100 meters and number of houses for which the nearest station is located in a radius of at least 1450 meters:

```{r,echo=bool}
nrow(a[a$station_distance<0.1 , ])
nrow(a[a$station_distance>1.45 , ])
```
alright. number of the samples are approximately equal.
now let's run a test to see if distance to nearest station has a effect on price or not:

H0 : mean( "d < 100m" ) = mean( "d > 1450m")

H1 : otherwise.

```{r,echo=bool}
tless = a[a$station_distance<0.1 ,]
tgreat = a[a$station_distance>1.45, ]

z.test(tless$price, y = tgreat$price , alternative = 'two.sided', mu = 0, sigma.x = sd(tless$price), sigma.y = sd(tgreat$price),conf.level = 0.95)
```

the p_value is really low and we can easily reject null hypothesis. it means that distance to the nearest station has an effect on the price of the house.

if we plot this data and also use regression method, we will got a plot like this :

```{r,echo=bool}

ndf = data.frame(c(tless$station_distance , tgreat$station_distance) , c(tless$price , tgreat$price) , stringsAsFactors = F)
colnames(ndf) = c("station_distance" , "price")
fit2 = lm(price ~ station_distance , data = ndf)
summary(fit2)

b0 = coef(fit2)[1]
b1 = coef(fit2)[2]


res = b0 +  b1 * ndf$station_distance
new_df2 = data.frame(ndf$station_distance , res , stringsAsFactors = F)
colnames(new_df2) = c("area" , "res")

ggplot(data = new_df2, aes(x = area, y = res))+geom_point(mapping = aes(ndf$station_distance , ndf$price))+ geom_line(color='red',data = new_df2, aes(x=area, y=res))
```

the slope of the line is negative which is obvious why.(the further a house is located to a station, the cheaper this house will be.)

### 5

remember this plot :


```{r,echo=bool}
t2 = a[a$surface_terrain!=0 , ]
hist(t2$surface_terrain , breaks = seq(0 , 1000 , 10) , col = "purple" , main="Distribution of surface_terrain" , xlab = "surface_terrain")
```

now let's calculate a 95% confidence interval for mean of the whole population.

```{r,echo=bool}
n = (dim(t2))[1]
sigma = sd(t2$surface_terrain)
xbar = mean(t2$surface_terrain)
upper = xbar + 1.960 * (sigma/sqrt(n))
lower = xbar - 1.960 * (sigma/sqrt(n))

print(paste("[" , lower , "," , upper , "]"))


```
ok. now let's test this result. we expect that the probability of mean of population  to be 250, is low. if we run a z.test :

```{r,echo=bool}
 z.test(t2$surface_terrain, y = NULL , alternative = 'two.sided', mu = 250, sigma.x = sd(t2$surface_terrain), sigma.y = NULL , conf.level = 0.95)

```
ok. the p_value is low nad shows that our expectation is right.


### 6

we expect that maisons have larger housing_surface than appartements. so we test this hypothesis :

H0 : mean of surface_housing in maisons is equal to  mean of surface_housing in appartements.

H1 : other wise.

```{r}
a$type_property = as.character(a$type_property)
r1 = a[a$type_property == "appartement" , ]
r2 = a[a$type_property == "maison" , ]

z.test(r2$surface_housing, y = r1$surface_housing , alternative = 'two.sided', mu = 0, sigma.x = sd(r2$surface_housing), sigma.y = sd(r1$surface_housing),conf.level = 0.95)
```
nice. the p-value is low and we can reject null hypothesis.
so as we expect mean of surface_housing in maisons is really larger than mean of surface_housing in appartements.


### 7

let's check if "area_price_ratio" and "district" has a relationship. we test this hypothesis by regression.
in regression the null hypothesis is that two variables has no relationships.

```{r}
fit5 = lm(area_price_ratio~district , data = a)
summary(fit5)
```

ok. the p_value of the test is really small. it means that there is a relationship between districts and area_price_ratio.
but this relation is small. as we have seen earlier on a map :

```{r,echo=bool}

ggplot()+geom_point(mapping = aes(x=a$latitude,y=a$longitude , color = a$area_price_ratio) )+scale_color_distiller(palette = 'Spectral')
```
here you can see that in the northeast of the city the prices are lower than center.
but in total, this realtionship is small.(we can refer to R^2 value of regression.)

### 8

let's have look at a previously plotted data :
```{r,echo=bool}
ggplot(df2, aes(x=year, y=price, group=district)) +
  geom_line(aes(color=district))+
  geom_point(aes(color=district))
```

so let's prepare for a regression.we are going to regress price of the house at time of transaction. suppose 2016/1 (jan) is set to be 1. we use data of district "Lyon 7e Arrondissement". (it can be done for any other districts too. just change variable "dis" in part below.)

```{r,echo=bool}
dis = "Lyon 7e Arrondissement"
new_df = a[a$district==dis,]
new_df$year = as.numeric(new_df$year)
new_df$month = as.numeric(new_df$month)
new_df$time = (new_df$year -2016)*12 + new_df$month
fit = lm(price~time , data = new_df)
summary(fit)

```

```{r,echo=bool}
b0 = coef(fit)[1]
b1 = coef(fit)[2]


res = b0 +  b1 * new_df$time
new_df2 = data.frame(new_df$time , res , stringsAsFactors = F)
colnames(new_df2) = c("time" , "res")

ggplot(data = new_df2, aes(x = time, y = res))+geom_point(mapping = aes(new_df$time , new_df$price))+ geom_point(color='yellow')+ geom_line(color='red',data = new_df, aes(x=time, y=res))
```

as we expect , the slope of the regresiion line is possitive and it shows that by passing time , price of houses are increased.


### 9
as we saw before :

```{r, echo=bool}
ggplot() + geom_point(mapping = aes(a$surface_housing , a$price))
```

```{r,echo=bool}
fit2 = lm(price ~ surface_housing , data = a)
summary(fit2)
```

as we can see, p-values are really small and t-values are really big, so we can reject null hypothesis of regression.(null hypothesis: there is no relationship between surface_housing and price.)

```{r,echo=bool}
b0 = coef(fit2)[1]
b1 = coef(fit2)[2]


res = b0 +  b1 * a$surface_housing
new_df2 = data.frame(a$surface_housing , res , stringsAsFactors = F)
colnames(new_df2) = c("area" , "res")

ggplot(data = new_df2, aes(x = area, y = res))+geom_point(mapping = aes(a$surface_housing , a$price))+ geom_line(color='red',data = new_df2, aes(x=area, y=res))
```

it seems good. we expect that a larger house cost more money and the slope of our regression line proved our expection.


### 10

we saw this in previous parts :
```{r, echo=bool}
r1 = (a[c("rooms_count")]==1)
r2 = (a[c("rooms_count")]==2)
r3 = (a[c("rooms_count")]==3)
r4 = (a[c("rooms_count")]==4)
r5 = (a[c("rooms_count")]==5)
r6 = (a[c("rooms_count")]==6)

ggplot() + geom_point(mapping = aes(a$surface_housing , a$rooms_count))+
    scale_y_continuous(breaks = c(0,1,2,3,4,5,6)) +
  geom_point(mapping = aes(mean(a$surface_housing[r1]) , 1) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r2]) , 2) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r3]) , 3) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r4]) , 4) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r5]) , 5) , color="red")+
  geom_point(mapping = aes(mean(a$surface_housing[r6]) , 6) , color="red")
```
let us rotate this :
```{r,echo=bool}
ggplot() + geom_point(mapping = aes(a$rooms_count , a$surface_housing))+
    scale_x_continuous(breaks = c(0,1,2,3,4,5,6)) +
  geom_point(mapping = aes(1, mean( a$surface_housing[r1]) ) , color="red")+
  geom_point(mapping = aes(2,mean( a$surface_housing[r2]) ) , color="red")+
  geom_point(mapping = aes(3,mean( a$surface_housing[r3]) ) , color="red")+
  geom_point(mapping = aes(4,mean( a$surface_housing[r4]) ) , color="red")+
  geom_point(mapping = aes(5, mean(a$surface_housing[r5]) ) , color="red")+
  geom_point(mapping = aes(6,mean( a$surface_housing[r6]) ) , color="red")
```



if we run regression on this set :

```{r,echo=bool}
fit2 = lm(surface_housing ~ rooms_count , data = a)
b0 = coef(fit2)[1]
b1 = coef(fit2)[2]


res = b0 +  b1 * a$rooms_count
new_df2 = data.frame(a$rooms_count , res , stringsAsFactors = F)
colnames(new_df2) = c("rooms_count" , "area")

ggplot(data = new_df2, aes(x = rooms_count, y = area))+geom_point(mapping = aes(a$rooms_count , a$surface_housing))+ geom_line(color='red',data = new_df2, aes(x=rooms_count, y=area)) 
```

so our assumption about increasing surface_housing when number of rooms increases is proved by regression.(slope of the line is posstive.)

### 11


we saw this in previous parts(I rotate it):

```{r, echo=bool}

r1 = (a[c("rooms_count")]==1)
r2 = (a[c("rooms_count")]==2)
r3 = (a[c("rooms_count")]==3)
r4 = (a[c("rooms_count")]==4)
r5 = (a[c("rooms_count")]==5)
r6 = (a[c("rooms_count")]==6)


ggplot() + geom_point(mapping = aes(a$rooms_count , a$price)) +
  scale_y_continuous(breaks = c(0,1,2,3,4,5,6))+ 
  geom_point(mapping = aes(1,mean(a$price[r1])) , color="red")+
  geom_point(mapping = aes(2,mean(a$price[r2]) ) , color="red")+
  geom_point(mapping = aes(3,mean(a$price[r3]) ) , color="red")+
  geom_point(mapping = aes(4,mean(a$price[r4]) ) , color="red")+
  geom_point(mapping = aes(5,mean(a$price[r5]) ) , color="red")+
  geom_point(mapping = aes(6,mean(a$price[r6]) ) , color="red")

```
if we run regression on this set :

```{r,echo=bool}
fit2 = lm(price ~ rooms_count , data = a)
b0 = coef(fit2)[1]
b1 = coef(fit2)[2]


res = b0 +  b1 * a$rooms_count
new_df2 = data.frame(a$rooms_count , res , stringsAsFactors = F)
colnames(new_df2) = c("rooms_count" , "price")

ggplot(data = new_df2, aes(x = rooms_count, y = price))+geom_point(mapping = aes(a$rooms_count , a$price))+ geom_line(color='red',data = new_df2, aes(x=rooms_count, y=price)) 
```
again, our assumption that price and rooms_count have positive correlation is proved by regression.


### 12

for houses with non-zero terrain, let's see if the area of the terrain affects the price(e.g price of the house increase if it has larger terrain):

first we select houses with non-zero terrain:
```{r,echo=bool}
rt = a[a$surface_terrain >0 , ]

fit2 = lm(price ~ surface_terrain , data = rt)
b0 = coef(fit2)[1]
b1 = coef(fit2)[2]


res = b0 +  b1 * rt$surface_terrain
new_df2 = data.frame(rt$surface_terrain , res , stringsAsFactors = F)
colnames(new_df2) = c("surface_terrain" , "price")

ggplot(data = new_df2, aes(x = surface_terrain, y = price))+geom_point(mapping = aes(rt$surface_terrain , rt$price))+ geom_line(color='red',data = new_df2, aes(x=surface_terrain, y=price)) 
```
```{r}
cor(rt$price , rt$surface_housing)
```

### 13

in PART I , we plotted a figure of changing "area_price_ratio" over time for each district. 

```{r, echo=FALSE}
districts = c("Lyon 1er Arrondissement" , "Lyon 2e Arrondissement" ,
              "Lyon 3e Arrondissement" , "Lyon 4e Arrondissement" ,
              "Lyon 5e Arrondissement" , "Lyon 6e Arrondissement" ,
              "Lyon 7e Arrondissement" , "Lyon 8e Arrondissement" ,
              "Lyon 9e Arrondissement" , "Villeurbanne")

years = c("2016" , "2017" , "2018" , "2019" , "2020" , "2021")
month = c("01" ,"02","03","04","05","06","07","08","09","10","11","12")

means = c()
dis = c()
yearss = c()
months = c()
for (di in districts) {
  for (ye in years) {
    for (mo in month) {
      t = mean(a[a$year==ye & a$month==mo & a$district ==di , 'area_price_ratio'])
      means = c(means , t)
      dis = append(dis , di)
      yearss = append(yearss , ye)
      months = append(months , mo)
    }
   
  }
}
yearss = as.numeric(yearss)
yearss = yearss - 2016
months = as.numeric(months)

time = 12 * yearss + months

nbv = data.frame(means , dis , yearss , months , time , stringsAsFactors = F)

nbv2 = nbv[complete.cases(nbv) , ]



ggplot(nbv2, aes(x=time, y=means, group=dis)) +
  geom_line(aes(color=dis))+
  geom_point(aes(color=dis))




```



now we choose one of the districts, then plot its points :

```{r,echo=bool}
spec_district = "Lyon 3e Arrondissement"
nbv3 = nbv2[nbv2$dis == spec_district , ]


ggplot(nbv3, aes(x=time, y=means, group=dis)) +
  geom_line(aes(color=dis))+
  geom_point(aes(color=dis))


```

again, let's run regression on these points :)

```{r,echo=bool}
fit4 = lm(means ~ time , data=nbv3)

b0 = coef(fit4)[1]
b1 = coef(fit4)[2]


res = b0 +  b1 * nbv3$time
new_df2 = data.frame(nbv3$time , res , stringsAsFactors = F)
colnames(new_df2) = c("time" , "area_price_ratio")

ggplot(data = new_df2, aes(x = time, y = area_price_ratio)) +geom_point(mapping = aes(nbv3$time , nbv3$means)) + geom_line(color='red',data = new_df2, aes(x=time, y=area_price_ratio)) +geom_line(mapping = aes(nbv3$time , nbv3$means))



# ggplot(data = new_df2, aes(x = time, y = price))+geom_point(mapping = aes(nbv3$time , nbv3$means))+ geom_line(color='red',data = new_df2, aes(x=surface_terrain, y=price)) 

```

```{r,echo=bool}
summary(fit4)
```
so as we deduced from previous parts, the prices are increased during time.(inflation.)

also this procedure can be applied to other districts too.




# part iii

let's see tha map(location of university is plotted as a purple dot) :
```{r,echo=bool}
a$district = as.factor(a$district)
ggplot(data = a)+geom_point(mapping = aes(x=latitude,y=longitude, color=district)) + geom_point(mapping = aes(45.780234113880425 , 4.865561717882041) , color="purple")
```

the university is located beside district "Villeurbanne". again, lets see price of land in districts :

```{r,echo=bool}
ggplot(df2, aes(x=year, y=price, group=district)) +
  geom_line(aes(color=district))+
  geom_point(aes(color=district))
```

great! not only are Villeurbanne's houses nearest to the university, but also this district has the lowest land price among all districts :)

so our search area for the house is just "Villeurbanne".

```{r,echo=bool}
vill = a[a$district=="Villeurbanne" ,]

ggplot()+geom_point(mapping = aes(x=vill$latitude,y=vill$longitude , color = vill$price) )+scale_color_distiller(name="price" ,palette = 'Spectral') + geom_point(mapping = aes(45.780234113880425 , 4.865561717882041) , color="purple") + xlab("latitude") + ylab("longitude")
```

nice! it seems some cheap houses are located near the university.
let's consider train stations in this area(red points) :

```{r,echo=bool}

AAB = stations
AAB$model = as.factor(AAB$model)
# colnames(AAB) = c("name" , "latitude" , "longitude" , "groups")


good = AAB[(45.79 > AAB$latitude)&(AAB$latitude > 45.746) & (4.85<AAB$longitude) &(AAB$longitude < 4.95) , ]
#
#
ggplot()+
  geom_point(mapping = aes(x=vill$latitude,y=vill$longitude))+
  geom_point(mapping = aes(45.780234113880425 , 4.865561717882041) , color="purple")+ geom_point(mapping = aes(good$latitude , good$longitude,color= "Red"))+xlab("latitude") + ylab("longitude")

```

Good! there some stations in this area. now I should be more specific for the features of a house i'm looking for. as a student, the most important factor is the price of the house. the second important factor is distance to university. the third important factor is distance to a station.

so i will define a cost function. with this cost function, I will test all houses in this area and then, i will select a house which has minimum cost for me.

first let's calculate the distance of each house from university :

```{r ,echo=bool}
  lat1 = a$latitude 
  lat2 = 45.780234113880425
  lat1 = lat1 / ((180/pi))
  lat2 = lat2 / ((180/pi))
  
  lon1 = a$longitude
  lon2 = 4.865561717882041
  
  lon1 = lon1 / ((180/pi)) 
  lon2 = lon2 / ((180/pi)) 
  
  d = 3963.0 * acos((sin(lat1) * sin(lat2)) + cos(lat1) * cos(lat2) * cos(lon2-lon1))
  d = 1609.344 * d

  a$uni_distance = d
  
  vill = a[a$district=="Villeurbanne" ,]
  vill$station_distance = vill$station_distance*1000
  vill$price = vill$price / 100
```

we scale price and distances. now they are in the same order.

now let's define our cost :

```{r ,echo=bool}
c1 = 3
c2 = 2
c3 = 1

vill$cost = c1*vill$price + c2*vill$uni_distance + c3*vill$station_distance
vill = vill[order(vill$cost),]
```

now if we plot the best case :
```{r,echo=bool}
ggplot()+
  geom_point(mapping = aes(x=vill$latitude,y=vill$longitude) , color="grey")+
  geom_point(mapping = aes(45.780234113880425 , 4.865561717882041) , color="purple")+ geom_point(mapping = aes(good$latitude , good$longitude,color= "Red"))+geom_point(mapping = aes(vill$latitude[1] , vill$longitude[1]) , color="blue")+xlab("latitude") + ylab("longitude")


```

OH YES! we found it.(we name this house as Beta House!) but let's have a look at other houses.(maybe they have higher costs, but they have other good feature which can seduce us to buy them!)


```{r , render = lemon_print,echo=bool}

vill2 = vill[1:30 , ]
vill2$price = vill2$price * 100
head(vill2 , 'all')
```

OK. this is the list of 30 house with least value of cost function.
among all this houses, our house has the minimum price.
but let's look at two other important features : distance to university and distance to nearest station.

if we look carefully, we noticed that there is only 1 house with better location versus Beta House . but, the price of this house is almost 3 times greater than Beta House. and as we mentioned in the begining, price of the house is the most important factor.

so, Beta house is the optimal chioce for me.

```{r , render = lemon_print ,echo=bool}
vill2[1,]
```
```{r , echo=FALSE}
ggplot()+
  geom_point(mapping = aes(x=vill$latitude,y=vill$longitude) , color="grey")+
  geom_point(mapping = aes(45.780234113880425 , 4.865561717882041) , color="purple")+ geom_point(mapping = aes(good$latitude , good$longitude,color= "Red"))+geom_point(mapping = aes(vill$latitude[1] , vill$longitude[1]) , color="blue")+xlab("latitude") + ylab("longitude")


```

gray dots are houses in Villeurbanne, red dots are stations in this area, blue point is "Beta House" and purple dot is the university.

but this house is sold before. what we can do is to search in the area near this house to find another optimal house to buy.

THE END! 







